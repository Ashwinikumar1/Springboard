{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:100%\" src=\"../images/practical_xgboost_in_python_notebook_header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting - Wisdom of the Crowd (practice)\n",
    "**This chapter includes**:\n",
    "- <a href=\"#data-preparation\">Data preparation</a>\n",
    "- <a href=\"#sdt\">Using single decision tree</a>\n",
    "- <a href=\"#ada\">Boosting with AdaBoost</a>\n",
    "- <a href=\"#gbt\">Gradient Boosted Trees - why not?</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's time to see how boosting is applied in practice. Hopefully the `scikit-learn` package provides all described packages. Begin with importing all required libraries. XGBoost package will be described more in later lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# reproducibility\n",
    "seed = 142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data <a name='data-preparation' />\n",
    "In all examples we will be dealing with **binary classification**.  Generate 20 dimensional artificial dataset with 1000 samples, where 8 features holding information, 3 are redundant and 2 repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=8, n_redundant=3, n_repeated=2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally perform a split into train/test parts. It will be useful for validating the performance of all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All algorithms won't be tuned at this point. A sensible set of default settings will be applied, making the whole things less complicated.\n",
    "\n",
    "> [*Logarithmic loss*](https://www.kaggle.com/wiki/LogarithmicLoss) and accuracy were chosen to evaluate the results. It's also important to remeber about reproducibility - you should always set all `seed` parameters to the same value.\n",
    "\n",
    "Let's perform a target variable distribution sanity check before digging into algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n",
      "Counter({1: 404, 0: 396})\n",
      "\n",
      "Test label distribution:\n",
      "Counter({0: 104, 1: 96})\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label distribution:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target variable is equally distribued across both dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Decision Tree <a name='sdt' />\n",
    "The following code will create a single decision tree, fit it using training data and evaluate the results using test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Decision Tree ==\n",
      "Accuracy: 0.84\n",
      "Log loss: 5.53\n",
      "Number of nodes created: 159\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=seed)\n",
    "\n",
    "# train classifier\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# predict output\n",
    "decision_tree_y_pred  = decision_tree.predict(X_test)\n",
    "decision_tree_y_pred_prob  = decision_tree.predict_proba(X_test)\n",
    "\n",
    "# evaluation\n",
    "decision_tree_accuracy = accuracy_score(y_test, decision_tree_y_pred)\n",
    "decision_tree_logloss = log_loss(y_test, decision_tree_y_pred_prob)\n",
    "\n",
    "print(\"== Decision Tree ==\")\n",
    "print(\"Accuracy: {0:.2f}\".format(decision_tree_accuracy))\n",
    "print(\"Log loss: {0:.2f}\".format(decision_tree_logloss))\n",
    "print(\"Number of nodes created: {}\".format(decision_tree.tree_.node_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see two things:\n",
    "\n",
    "1. the log loss score is not very promising (due to the fact that leaves in decision tree outputs either `0` or `1` as probability which is heaviliy penalized in case of errors, but the accuracy score is quite decent,\n",
    "2. the tree is complicated (large number of nodes)\n",
    "\n",
    "You can inspect first few predicted outputs, and see that only 2 instances out of 5 were classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:\n",
      "[0 0 1 0 1]\n",
      "\n",
      "Predicted labels:\n",
      "[1 1 1 1 0]\n",
      "\n",
      "Predicted probabilities:\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('True labels:')\n",
    "print(y_test[5:10,])\n",
    "print('\\nPredicted labels:')\n",
    "print(decision_tree_y_pred[:5,])\n",
    "print('\\nPredicted probabilities:')\n",
    "print(decision_tree_y_pred_prob[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost <a name='ada' />\n",
    "In the example below we are creating a AdaBoost classifier running on 1000 iterations (1000 trees created). Also we are growing decision node up to first split (they are called *decision stumps*). We are also going to use `SAMME` algorithm which is inteneded to work with discrete data (output from `base_estimator` is `0` or `1`). Please refer to the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) and [here](http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== AdaBoost ==\n",
      "Accuracy: 0.82\n",
      "Log loss: 0.69\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    algorithm='SAMME',\n",
    "    n_estimators=1000,\n",
    "    random_state=seed)\n",
    "\n",
    "# train classifier\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# calculate predictions\n",
    "adaboost_y_pred = adaboost.predict(X_test)\n",
    "adaboost_y_pred_prob = adaboost.predict_proba(X_test)\n",
    "\n",
    "# evaluate\n",
    "adaboost_accuracy = accuracy_score(y_test, adaboost_y_pred)\n",
    "adaboost_logloss = log_loss(y_test, adaboost_y_pred_prob)\n",
    "\n",
    "print(\"== AdaBoost ==\")\n",
    "print(\"Accuracy: {0:.2f}\".format(adaboost_accuracy))\n",
    "print(\"Log loss: {0:.2f}\".format(adaboost_logloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-loss metrics is much lower than in single decision tree (mainly to the fact that now we obtain probabilities output). The accuracy is the same, but notice that the structure of the tree is much simpler. We are creating 1000 **decision tree stumps**.\n",
    "\n",
    "Also here a quick peek into predicted values show that now 4 out of 5 first test instances are classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:\n",
      "[1 1 1 0 0]\n",
      "\n",
      "Predicted labels:\n",
      "[1 1 1 0 0]\n",
      "\n",
      "Predicted probabilities:\n",
      "[[0.49742697 0.50257303]\n",
      " [0.50076613 0.49923387]\n",
      " [0.5016833  0.4983167 ]\n",
      " [0.50282961 0.49717039]\n",
      " [0.50747316 0.49252684]]\n"
     ]
    }
   ],
   "source": [
    "print('True labels:')\n",
    "print(y_test[:5,])\n",
    "print('\\nPredicted labels:')\n",
    "print(adaboost_y_pred[:5,])\n",
    "print('\\nPredicted probabilities:')\n",
    "print(adaboost_y_pred_prob[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for clarity, let's check how the first tree looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's it's error and contribution into final ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.38\n",
      "Tree importance: 0.47\n"
     ]
    }
   ],
   "source": [
    "print(\"Error: {0:.2f}\".format(adaboost.estimator_errors_[0]))\n",
    "print(\"Tree importance: {0:.2f}\".format(adaboost.estimator_weights_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees<a name='gbt' />\n",
    "Let's construct a gradient boosted tree consiting of 1000 trees where each successive one will be created with gradient optimization. Again we are going to leave most parameters with their default values, specifiy only maximum depth of the tree to 1 (again decision stumps), and setting warm start for more intelligent computations. Please refer to the [docs](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) if something is not clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Gradient Boosting ==\n",
      "Accuracy: 0.80\n",
      "Log loss: 0.46\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(\n",
    "    max_depth=1,\n",
    "    n_estimators=1000,\n",
    "    warm_start=True,\n",
    "    random_state=seed)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "gbc_y_pred = gbc.predict(X_test)\n",
    "gbc_y_pred_prob = gbc.predict_proba(X_test)\n",
    "\n",
    "# calculate log loss\n",
    "gbc_accuracy = accuracy_score(y_test, gbc_y_pred)\n",
    "gbc_logloss = log_loss(y_test, gbc_y_pred_prob)\n",
    "\n",
    "print(\"== Gradient Boosting ==\")\n",
    "print(\"Accuracy: {0:.2f}\".format(gbc_accuracy))\n",
    "print(\"Log loss: {0:.2f}\".format(gbc_logloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained results are obviously the best of all presented algorithm. We have obtained most accurate algorithm giving more sensible predictions about class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:\n",
      "[1 1 0 0 0]\n",
      "\n",
      "Predicted labels:\n",
      "[1 0 0 0 0]\n",
      "\n",
      "Predicted probabilities:\n",
      "[[0.12344095 0.87655905]\n",
      " [0.69217302 0.30782698]\n",
      " [0.83598136 0.16401864]\n",
      " [0.9159873  0.0840127 ]\n",
      " [0.99863653 0.00136347]]\n"
     ]
    }
   ],
   "source": [
    "print('True labels:')\n",
    "print(y_test[:5,])\n",
    "print('\\nPredicted labels:')\n",
    "print(gbc_y_pred[:5,])\n",
    "print('\\nPredicted probabilities:')\n",
    "print(gbc_y_pred_prob[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is that GBC uses `DecisionTreeRegressor` classifier as the estimator with *mean-square error* as criterion. This results of slightly different output of the tree - now the leaf contains a predicted value (while the first splitting point remains the same)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
